setwd("C:\\Users\\Chowdary\\Desktop\\DataMining\\DM Assignment4")
getwd()
train = read.csv("sonar_train.csv",header = FALSE)
test = read.csv("sonar_test.csv",header = FALSE)
summary(train)
dim(train)
summary(test)
dim(test)
library(rpart)
library(rpart.plot)
help("rpart.control")
help("rpart.plot")
x <- train[,1:60]
y <- as.factor(train[,61])
model <- rpart(y~.,x,control=rpart.control(minsplit=0,minbucket=0,cp=-1, maxcompete=0, maxsurrogate=0, usesurrogate=0, xval=0,maxdepth=5))
rpart.plot(model, box.palette="RdBu", shadow.col="orange", nn=TRUE,border="red",col="black")
x_test <- test[,1:60]
y_test <- as.factor(test[,61])
1 - sum(y_test == predict(model,x_test,type = "class")) / length(y_test)
setwd("C:\\Users\\Chowdary\\Desktop\\DataMining\\DM Assignment4")
#install.packages("randomForest")
library("randomForest")
train<-read.csv("sonar_test.csv",header=FALSE)
test<-read.csv("sonar_test.csv",header=FALSE)
x_train=train[,1:60]
y_train=as.factor(train[,61])
x_test=test[,1:60]
y_test=as.factor(test[,61])
model<-randomForest(x_train,y_train)
1-sum(y_train==predict(model,x_train))/length(y_train)
setwd("C:\\Users\\Chowdary\\Desktop\\DataMining\\DM Assignment4")
#install.packages("class")
library(class)
train<-read.csv("sonar_test.csv",header=FALSE)
test<-read.csv("sonar_test.csv",header=FALSE)
x_train=train[,1:60]
y_train=as.factor(train[,61])
x_test=test[,1:60]
y_test=as.factor(test[,61])
help("knn")
model1<-knn(x_train,x_test,y_train,k=5)
1-sum(y_test==model1)/length(y_test)
model2<-knn(x_train,x_test,y_train,k=6)
1-sum(y_test==model2)/length(y_test)
setwd("C:\\Users\\Chowdary\\Desktop\\DataMining\\DM Assignment4")
#install.packages("class")
library(class)
train<-read.csv("sonar_test.csv",header=FALSE)
test<-read.csv("sonar_test.csv",header=FALSE)
x_train=train[,1:60]
y_train=as.factor(train[,61])
x_test=test[,1:60]
y_test=as.factor(test[,61])
help("knn")
model1<-knn(x_train,x_test,y_train,k=5)
1-sum(y_test==model1)/length(y_test)
model2<-knn(x_train,x_test,y_train,k=6)
1-sum(y_test==model2)/length(y_test)
#install.packages("class")
library(class)
train<-read.csv("sonar_test.csv",header=FALSE)
test<-read.csv("sonar_test.csv",header=FALSE)
x_train=train[,1:60]
y_train=as.factor(train[,61])
x_test=test[,1:60]
y_test=as.factor(test[,61])
help("knn")
model1<-knn(x_train,x_test,y_train,k=5)
1-sum(y_test==model1)/length(y_test)
model2<-knn(x_train,x_test,y_train,k=6)
1-sum(y_test==model2)/length(y_test)
setwd("C:\\Users\\Chowdary\\Desktop\\Final exam\\Final exam")
bse_sensex <- read.csv("BSE_Sensex_Index.csv")
SGR_Close <- c()
for (i in 1:15446){
SGR_Close[i] <- (bse_sensex$Close[i] - bse_sensex$Close[i+1]) / bse_sensex$Close[i+1]
}
SGR_Close[15447] <- (SGR_Close[15446] + SGR_Close[15445] + SGR_Close[15444]) / 3
SGR_Close[15447]
Z_SGR_Close <- c()
mean_SGR_Close <- mean(SGR_Close)
mean_SGR_Close
sd_SGR_Close <- sd(SGR_Close)
sd_SGR_Close
for (j in 1:15447) {
Z_SGR_Close[j] <- (SGR_Close[j] - mean_SGR_Close) / (sd_SGR_Close)
}
outliers_dates <- c()
outliers_count <- 0
otd <- 1
for (k in 1:15447) {
if (Z_SGR_Close[k] > 3) {
outliers_count <- outliers_count + 1
outliers_dates[otd] <- bse_sensex$Date[k]
otd <- otd + 1
}
if (Z_SGR_Close[k] < -3) {
outliers_count <- outliers_count + 1
outliers_dates[otd] <- bse_sensex$Date[k]
otd <- otd + 1
}
}
outliers_count
outliers_dates
setwd("G:\\DataScience_2019501068\\Data-Mining\\Exam-Solutions\\Final exam\\Final exam\\question-2")
data = read.csv("apriori_data.csv", header = TRUE);
View(data)
data$TID <- NULL
library(arules)
write.csv(data, "ItemList.csv", quote = FALSE, row.names = TRUE)
transactions = read.transactions("ItemList.csv", sep=',', rm.duplicates = TRUE)
inspect(transactions)
frequent_itemsets <- apriori(transactions, parameter = list(sup = 0.03, conf = 0.5,target="frequent itemsets"))
inspect(sort(frequent_itemsets)[1:15])
itemFrequencyPlot(transactions, topN = 5, col="red")
setwd("G:\\DataScience_2019501068\\Data-Mining\\Exam-Solutions\\Final exam\\Final exam\\question-3")
lensdata = read.csv("lenses.data.csv", header = FALSE, col.names = c("index", "age", "spectacle_prescription", "astigmatic", "tear_production_rate", "Class"))
lensdata$index <- NULL
library(rpart)
y<-as.factor(lensdata[,5])
x<-lensdata[,1:4]
model1<-rpart(y~.,x, parms = list(split = 'information'),
control=rpart.control(minsplit=0,minbucket=0,cp=-1, maxcompete=0, maxsurrogate=0, usesurrogate=0, xval=0,maxdepth=5))
library(rpart.plot)
rpart.plot(model1)
gain <- sum(y==predict(model1,x,type="class"))/length(y)
gain
error_rate <- 1-sum(y==predict(model1,x,type="class"))/length(y)
error_rate
setwd("G:\\DataScience_2019501068\\Data-Mining\\Exam-Solutions\\Final exam\\Final exam")
data = read.csv("Liver_data.csv", header = FALSE, col.names = c("mcv", "alkphos", "sgpt", "sgot", "gammagt", "drinks", "selector"))
data$drinks = cut(data$drinks, breaks = c(0,5,10,15,20,25), labels = c('C1', 'C2', 'C3', 'C4', 'C4'), right = FALSE)
data = na.omit(data)
traindata = subset(data, data$selector == 1)
testdata = subset(data, data$selector == 2)
x_train <- subset(traindata, select = -c(selector, drinks))
x_test <- subset(testdata, select = -c(selector, drinks))
y_train = traindata[,6, drop = TRUE]
y_test = testdata[,6, drop = TRUE]
library(class)
model1 = knn(x_train, x_test, y_train, k = 1)
1-sum(y_train==model1)/length(y_train)
model2 = knn(x_train, x_train, y_train, k = 2)
1-sum(y_train==model2)/length(y_train)
model3 = knn(x_train, x_train, y_train, k = 3)
1-sum(y_train==model3)/length(y_train)
setwd("C:\\Users\\Chowdary\\Desktop\\Final exam\\Final exam")
liver = read.csv("Liver_data.csv", header = FALSE, col.names = c("mcv", "alkphos", "sgpt", "sgot", "gammagt", "drinks","selector"))
liver$selector <- as.factor(liver$selector)
liver$drinks <- cut(liver$drinks, breaks = c(0, 5,10,15,20),
labels = c('C1', 'C2', "C3", 'C4'), right = FALSE)
liver <- na.omit(liver)
train = subset(liver, liver$selector == 1)
test = subset(liver, liver$selector == 2)
x_train <- subset(train, select = -c(selector, drinks))
x_test <- subset(test, select = -c(selector, drinks))
library(class)
y_train = train[,6, drop = TRUE]
y_test = test[,6, drop = TRUE]
library(e1071)
fit = svm(x_train, y_train)
1-sum(y_train==predict(fit,x_train))/length(y_train)
fit = svm(x_test, y_test)
1-sum(y_test==predict(fit,x_test))/length(y_test)
setwd("C:\\Users\\Chowdary\\Desktop\\Final exam\\Final exam")
bse_sensex <- read.csv("BSE_Sensex_Index.csv")
SGR_Close <- c()
for (i in 1:15446){
SGR_Close[i] <- (bse_sensex$Close[i] - bse_sensex$Close[i+1]) / bse_sensex$Close[i+1]
}
SGR_Close[15447] <- (SGR_Close[15446] + SGR_Close[15445] + SGR_Close[15444]) / 3
SGR_Close[15447]
Z_SGR_Close <- c()
mean_SGR_Close <- mean(SGR_Close)
mean_SGR_Close
sd_SGR_Close <- sd(SGR_Close)
sd_SGR_Close
for (j in 1:15447) {
Z_SGR_Close[j] <- (SGR_Close[j] - mean_SGR_Close) / (sd_SGR_Close)
}
outliers_dates <- c()
outliers_count <- 0
otd <- 1
for (k in 1:15447) {
if (Z_SGR_Close[k] > 3) {
outliers_count <- outliers_count + 1
outliers_dates[otd] <- bse_sensex$Date[k]
otd <- otd + 1
}
if (Z_SGR_Close[k] < -3) {
outliers_count <- outliers_count + 1
outliers_dates[otd] <- bse_sensex$Date[k]
otd <- otd + 1
}
}
outliers_count
outliers_dates
